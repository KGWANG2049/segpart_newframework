wandb:
  enable: true
  api_key: 7e17064baa2aaa0cccea0a2159ad51c758b7bf3a  # your wandb api key
  entity: ies_kaige  # the place to save your runs. can be your wandb username or team name
  project: ma_kaige # the name of your project
  name: seg_serD_downup1111_att1111_lastestup_max_ave_local_vector_sub_wdecay1e-4
train:  # only valid when running the training script
  epochs: 200
  lr: 1e-4
  lr_scheduler:
    enable: true
    which: cosLR  # expLR, stepLR, cosLR or cos_warmupLR
    expLR:
      gamma: 0.95
    stepLR:
      gamma: 0.2  # lr = gamma * lr, when decay step is hit
      decay_step: 50
    cosLR:
      T_max: ${train.epochs}  # maximum epochs
      eta_min: 1e-8  # minimum lr
    cos_warmupLR:
      warmup_epochs: 10  # number of epochs the warmup process takes
      warmup_init_lr: ${train.lr_scheduler.cos_warmupLR.eta_min}  # initial warmup lr
      T_max: 190  # number of epochs the cosine annealing process takes. should be epochs - warmup_epochs
      eta_min: 1e-8  # minimum lr of cosine annealing process
  optimizer:
    which: adamw  # adamw or sgd
    weight_decay: 1e-4
  consistency_loss_factor: 1  # only valid when training modelnet model
  validation_freq: 1  # frequency in epoch(s) to validate the model
  label_smoothing: false
  epsilon: 0.2  # epsilon for label smoothing
  dataloader:
    selected_points: 2048  # points to be selected from every point cloud
    fps: false  #  whether to use fps to select points. if false, use random downsample to select points
    combine_trainval: true  # combine train and validation set as train set
    batch_size_per_gpu: 8  # the actual batch size should be batch_size_per_gpu * num_gpu
    num_workers: ${train.ddp.nproc_this_node}  # the number of subprocess to load data
    prefetch: ${train.dataloader.batch_size_per_gpu}  # samples to be prefetched. e.g. 64 means 64*num_workers samples to be prefetched
    pin_memory: true  # pin memory in RAM

    data_augmentation:
      enable: true
      num_aug: 1  # how many augmentations applied in one point cloud at the same time
      jitter:  # add Gaussian noise to point cloud xyz positions
        enable: true
        std: 0.01
        clip: 0.05
      rotate:
        enable: true
        which_axis: y
        angle_range: [-15, 15]  # the unit is degree
      translate:
        enable: true
        x_range: [-0.2, 0.2]
        y_range: [-0.2, 0.2]
        z_range: [-0.2, 0.2]
      anisotropic_scale:
        enable: true
        x_range: [0.66, 1.5]
        y_range: [0.66, 1.5]
        z_range: [0.66, 1.5]
        isotropic: false
  ddp:
    which_gpu: [2]
    syn_bn: true  # synchronize batch normalization among gpus
    master_addr: localhost  # don't change this if you use only one PC
    master_port: 12346  # please choose an available port
    nnodes: 1  # how many PCs you want to use
    nproc_this_node: 1  # how many gpu you want to use in current PC, should match 'which_gpu'
    rank_starts_from: 0  # don't change this if you use only one PC
    world_size: 1  # this is equal to 'nproc_this_node' if you only use one PC
  amp: false  # whether to use automatic mixed precision
  grad_clip:
    enable: false
    mode: value  # clip by value or by norm
    max_norm: 1e-2
    value: 1e-2
  debug:
    enable: false
    check_layer_input_range: true
    check_layer_output_range: true
    check_layer_parameter_range: true
    check_gradient_input_range: true  # gradient w.r.t layer input
    check_gradient_output_range: true  # gradient w.r.t layer output
    check_gradient_parameter_range: true  # gradient w.r.t layer parameters

test:  # only valid when running the test script
  label_smoothing: false
  epsilon: 0.2  # epsilon for label smoothing
  dataloader:
    batch_size_per_gpu: 4
    num_workers: ${test.ddp.nproc_this_node}  # the number of subprocess to load data
    prefetch: ${test.dataloader.batch_size_per_gpu}  # samples to be prefetched. e.g. 64 means 64*num_workers samples to be prefetched
    pin_memory: true  # pin memory in RAM
    vote:
      enable: true
      num_vote: 10
      vote_start_epoch: 150
  ddp:
    which_gpu: [2]
    master_addr: localhost  # don't change this if you use only one PC
    master_port: 12346  # please choose an available port
    nnodes: 1  # how many PCs you want to use
    nproc_this_node: 1  # how many gpu you want to use in current PC, should match 'which_gpu'
    rank_starts_from: 0  # don't change this if you use only one PC
    world_size: 1  # this is equal to 'nproc_this_node' if you only use one PC
  print_results: true
  visualize_preds:  # only be used in apes
    enable: true
    format: png  # png or ply
    vis_which: [0, 4]  # which category to be visualized
    num_vis: 5  # how many point clouds to visualize for one category
  visualize_attention_heatmap:  # only be used in apes
    enable: true
    format: png  # png or ply
    vis_which: [0, 4]  # which category to be visualized
    num_vis: 5  # how many point clouds to visualize for one category


Point_Embedding:
  point_emb1_in: [3]
  point_emb1_out: [16]
  point_emb2_in: [16]
  point_emb2_out: [32]

Downsample:
  points_select_basis: ['xyz', 'xyz', 'xyz']  # xyz or feature
  n_samples: [512, 128, 32] # List for each downsample layer
  embedding_k: [32, 32, 32]
  conv1_channel_in: [64, 128, 256]
  conv1_channel_out: [64, 128, 256]
  conv2_channel_in: [64, 128, 256]
  conv2_channel_out: [64, 128, 256]


Local_CrossAttention_layer:
  enable: true
  pe_method: ['false', 'false', 'false']
  single_scale_or_multi_scale: ['ss', 'ss', 'ss']  # need input 'ss' or 'ms'
  key_one_or_sep: ['one', 'one', 'one']  # need input  'one' or 'sep', when single_scale_or_multi_scale = ss, only 'one' is Effective
  shared_ca: [false, false, false]  # it must be false when key_one_or_sep = one
  K: [32, 16, 8]
  scale: [2, 2, 2]  # represent different meanings. when single_scale_or_multi_scale is ss, 2 means single 2, when ms 2 means use scale 0 & 1 & 2
  neighbor_selection_method: ['feature', 'feature', 'feature']  # feature or coordinate
  neighbor_type: ['center_neighbor', 'center_neighbor', 'center_neighbor']  # diff or neighbor or center_diff or center_neighbor or neighbor_diff or center_neighbor_diff
  mlp_or_sum: ['sum', 'sum', 'sum']  # its useful only when single_scale_or_multi_scale = ms, and key_one_or_sep = sep
  q_in: [64, 128, 256]
  q_out: [64, 128, 256]
  k_in: [64, 128, 256]
  k_out: [64, 128, 256]
  v_in: [64, 128, 256]
  v_out: [64, 128, 256]
  num_heads: [4, 4, 4]
  att_score_method: ['local_vector_sub', 'local_vector_sub', 'local_vector_sub']  # local_scalar_dot or local_scalar_sub or local_scalar_add or local_scalar_cat or local_vector_sub or local_vector_add
  ff_conv1_channels_in: [64, 128, 256]                                                          # global_dot or global_sub
  ff_conv1_channels_out: [256, 512, 1024]  #input x 4
  ff_conv2_channels_in: [256, 512, 1024]
  ff_conv2_channels_out: [64, 128, 256]

UpSampleInterpolation:
  v_dense: [128, 64, 32]
  up_k: [4, 4, 4]
